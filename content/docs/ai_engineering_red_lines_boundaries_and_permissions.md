---
title: AI 系统工程的生死线：边界、权限与伪智能
description: 深入探讨 AI Agent 系统架构中的边界设计、权限控制和工程红线。揭示为什么绝大多数 AI Agent 项目失败的根本原因，以及如何构建工程合格的 AI 系统。
---

![AI ](/images/ai_engineering_red_lines_boundaries_and_permissions.png)

**先给结论：**

绝大多数 AI Agent 项目的“烂尾”，不是因为模型不够聪明，而是因为架构师在一开始就**把方向搞反了**。
在传统的软件工程里，我们追求 100% 的确定性；而引入 AI，本质上是在向精密系统中引入一个**“概率性组件”**。

如果你试图让一个概率性组件去承担确定性的责任，这不叫创新，这叫**系统架构级的玩忽职守**。

---

## 一、 一个被严重低估的事实：AI 不配“负责”

在设计 Agent 时，我们常犯的一个错误是：默认“既然它能理解语义，那它就能掌控流程”。
这是一个极其危险的假设。

从工程视角看，LLM（大语言模型）具备三个天然缺陷：

1.  **非确定性（Non-deterministic）：** 同样的输入，可能产生微偏的输出。
2.  **不可验证性（Unverifiable）：** 你很难用单元测试去覆盖所有逻辑分支。
3.  **难以复现（Hard to Reproduce）：** 线上出的 Bug，本地可能跑一百次都复现不出来。

这三个属性，与“责任（Accountability）”二字天然互斥。
**结论很残酷：AI 不适合当决策者（Decision Maker），它只配当执行器（Executor）。**

---

## 二、 三道绝对不可跨越的“工程红线”

这是用无数次生产环境事故换来的教训。无论你的 Prompt 写得有多精妙，以下三类权限，**绝对不能**直接交给 AI。

### 1. 价值判断与风险裁决（Gatekeeping）

**场景：** 内容审核通过、贷款审批、终止服务。

千万不要让 AI 直接输出 `True` 或 `False` 来决定业务走向。
模型的本质是“文字接龙”，它给出的是**“概率上最像答案的答案”**，而不是**“逻辑上绝对正确的真理”**。

- **错误做法：** AI 判断用户违规 $\rightarrow$ 直接触发封号接口。
- **正确做法：** AI 输出违规概率 + 风险标签 $\rightarrow$ 传统规则引擎（Rule Engine）复核。

**记住：价值判断必须是二元且确定的。哪怕规则写得粗糙一点，也比模棱两可的“智能”要安全。**

### 2. 状态的所有权（State Ownership）

这是最隐蔽的“炸弹”。很多开发者习惯在 Prompt 里维护一个 Session，让 AI 去记录“现在进行到第几步了”。

**这是一个架构级的反模式。**
一旦 AI 在多轮对话中产生了幻觉，或者遗忘了上下文，你的业务状态就“丢了”。
不可回放的状态 = 不可 Debug 的系统。

**铁律：状态机（FSM）必须外置。**
AI 只能**“读取”**状态，没有资格**“持有”**状态。

### 3. 不可回滚的“落锤”操作（Irreversible Actions）

**场景：** `DELETE` 数据库、大额转账、发送无法撤回的邮件。

**正确的工程姿势只有一个：AI 只能生成 Proposal（提议），不能执行 Commit（提交）。**
如果你允许 AI 直接操作现实世界，你不是在做系统，你是在**用概率函数去赌博**。

---

## 三、 理想的 Agent 架构：三明治模型

若要构建一个工程合格的系统，请参照这个结构自检：

1.  **上层：决策层（Deterministic Rules）**
    - _谁说了算？_ 传统代码、状态机、策略引擎。
2.  **中层：编排层（Orchestration）**
    - _谁在跑？_ 流程控制器。
3.  **底层：执行层（The AI Engine）**
    - _谁在干活？_ LLM。

**架构原则：决策永远在 AI 之外。AI 越是处于系统的“最底层”，系统越稳定。**

---

## 四、 所有的逻辑，都依赖于“记忆”

讲完了 Agent 的“手脚”（执行边界），我们必须面对另一个更严峻的问题：Agent 的“大脑”（知识获取）。

你可能搭建了完美的 Agent 架构，划分了清晰的边界，但系统依然可能给出一堆废话。
**为什么？因为你喂给它的数据是脏的。**

这就触及到了 AI 工程的另一大深水区——**RAG（检索增强生成）**。

现在很多团队做 RAG，以为把文档切碎了扔进向量数据库就万事大吉了。
结果就是：

- 用户问：“支付失败怎么办？”
- AI 回答：“支付成功的定义是……”（因为向量相似度认为“失败”和“成功”很像）。

**Garbage In, Garbage Out.**
如果检索系统给 AI 喂的是噪音，最强的 Agent 也会变成最一本正经的“胡说八道生成器”。

RAG 的坑，比 Agent 的边界设计还要深：

- 为什么向量检索经常失效？
- 为什么切片（Chunking）不是简单的切字符串？
- 为什么你的知识库实际上是一个“随机数生成器”？

鉴于篇幅，关于**“如何构建一个真正工业级的 RAG 系统”**，以及**“为什么你的知识库总是检索不到重点”**，这需要单独一篇文章来深度拆解。

---

## 结语

我现在评估一个 AI 系统是否值得上线，只问一个核心问题：

> **“当它做错了（注意，是当它做错，而不是如果），我的系统能不能兜得住？”**

如果答案是“不能”，那这个 Agent 就不是资产，而是负债。
AI 的真正价值，从来不在能力上限，而在边界之内的稳定输出。

**下期内容**
[深度拆解 RAG：为什么你的知识库变成了"垃圾检索系统"？](./rag.md)
